{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03e29797-7bf2-46bb-83f9-23d876851aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "from huggingface_hub import InferenceClient\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bd31cfa-99e5-4665-93e7-df2196420a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = OpenAI()\n",
    "# os.environ\n",
    "\n",
    "repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    ")\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "\n",
    "    response = inference_client.post(\n",
    "        json={\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"max_new_tokens\": 200},\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return json.loads(response.decode())[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c43c4ba-f7af-4aef-ab2f-737a061b4f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write me a crazy joke\n",
      "Here's a crazy joke for you:\n",
      "\n",
      "Why did the chicken go to the therapist?\n",
      "\n",
      "Because it had a fowl temper and was having a egg-xistential crisis!\n",
      "\n",
      "Hope that made you cluck with laughter! \n",
      "\n",
      "(Note: I know it's a bit of a groaner, but hey, that's what makes it crazy, right?) \n",
      "\n",
      "Would you like me to come up with another one? \n",
      "\n",
      "(Also, if you have any specific topics or themes you'd like the joke to be about, feel free to let me know and I'll try to come up with something!) \n",
      "\n",
      "Let me know! \n",
      "\n",
      "(And if you're feeling brave, I can try to come up with an even crazier joke!) \n",
      "\n",
      "Just let me know! \n",
      "\n",
      "(And if you're feeling REALLY brave, I can try to come up with a joke that's so crazy, it'll make your head spin!) \n",
      "\n",
      "Just let me know! \n",
      "\n",
      "(And if you're feeling REALLY, REALLY\n"
     ]
    }
   ],
   "source": [
    "response = call_llm(llm_client, \"write me a crazy joke\")\n",
    "print (response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3c2fcb-4b65-43f6-a41f-f92041641a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
